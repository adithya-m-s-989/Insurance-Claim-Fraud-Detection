{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Insurance_Claim.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1Y_rHaNtM9EkVlPbyOpaGnHmO0FpBwchH\n",
        "\n",
        "###**Detecting Insurance Claim Fraud with Machine Learning**\n",
        "\n",
        "This project aims to detect fraudulent insurance claims using the \u2018insurance_claims.csv\u2019 dataset. It involves data preprocessing, feature selection, and applying supervised classification algorithms to predict fraud likelihood, improving fraud detection efficiency for insurance providers.\n",
        "\"\"\"\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report # Import confusion_matrix and classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "#Load the Dataset File\n",
        "insurance_df = pd.read_csv('insurance_claims.csv')\n",
        "insurance_df.head()\n",
        "\n",
        "\"\"\"####**Data Preprocessing**\"\"\"\n",
        "\n",
        "# View features, their data types and count of null values\n",
        "insurance_df.info()\n",
        "\n",
        "\"\"\"**Data Information Summary:**\n",
        "\n",
        "\n",
        "The dataset consists of 1,000 customer records with 38 features and one target variable. The features encompass customer details, policy information, incident specifics, and a fraud indicator.\n",
        "\n",
        "Upon inspection, the \u2018authorities_contacted\u2019 column contains 91 missing values, while the \u2019_c39\u2019 column appears to have no values and can be safely dropped.\n",
        "\n",
        "The other columns did not have any errors or missing values.\n",
        "\"\"\"\n",
        "\n",
        "# Drop c_39 column\n",
        "insurance_df.drop(['_c39'], axis=1, inplace=True)\n",
        "\n",
        "\"\"\"**Handling Null values in 'Authorities Contacted':**\n",
        "\n",
        "\n",
        "Upon closer inspection, the \u2018Authorities Contacted\u2019 column does not have actual null values; instead, the value \u201cNone\u201d was misinterpreted as NaN. To correct this, NaN values were manually replaced with \u201cNone\u201d to retain accurate data representation.\n",
        "\"\"\"\n",
        "\n",
        "#Replace NaN with \"None\" in authorities contacted column\n",
        "insurance_df.fillna({'authorities_contacted':'None'}, inplace=True)\n",
        "null_count = insurance_df['authorities_contacted'].isna().sum()\n",
        "print(f\"Number of null values in 'Authorities Contacted': {null_count}\")\n",
        "\n",
        "# Summary of numerical columns in data\n",
        "insurance_df.describe()\n",
        "\n",
        "\"\"\"####**Exploratory Data Analysis**\n",
        "\n",
        "**Feature Distribution**\n",
        "\"\"\"\n",
        "\n",
        "# Drop numerical columns that do not provide meaningful business insights when plotted as distributions\n",
        "df_filtered = insurance_df.drop(columns=['policy_number', 'insured_zip', 'witnesses'])\n",
        "\n",
        "# Plot histogram for all numerical columns\n",
        "df_filtered.hist(figsize=(20,20))\n",
        "plt.show()\n",
        "\n",
        "\"\"\"General Insights from Histograms of Insurance Claims Data:\n",
        "\n",
        "1. Claim Amount:\n",
        "\n",
        "\n",
        "Right-skewed Distribution: The histogram of claim amounts is right-skewed, indicating that most claims are for smaller amounts, with a few larger claims driving the tail of the distribution.\n",
        "\n",
        "\n",
        "\n",
        "2. Age:\n",
        "\n",
        "Roughly Normal or Bimodal Distribution: The histogram of insured age is observed to be a bimodal distribution which indicates distinct customer segments (e.g., younger and older drivers).\n",
        "\n",
        "Impact on Risk: Age is an important factor in assessing risk, as younger and older drivers may have higher accident rates compared to middle-aged drivers.\n",
        "\n",
        "\n",
        "3. Policy Deductible & Policy Annual Premium:\n",
        "\n",
        "Policy deductible is $500, $1,000 or $2,000.\n",
        "\n",
        "The policy annual premium appears to be normally distributed with a mean of about $1,256.\n",
        "\n",
        "\n",
        "4. Umbrella Limit, Capital Gains & Capital Loss:\n",
        "\n",
        "A significant proportion of customers have an umbrella limit, capital gain, or capital loss of $0, indicating that these financial protections or transactions are either uncommon or not utilized by most policyholders. This trend could suggest a lower prevalence of high-net-worth individuals in the dataset or a general preference for minimal risk coverage.\n",
        "\n",
        "5. Claim Distribution:\n",
        "\n",
        "Total Claim is the sum of vehicle claim, injury claim, and property claim. The distribution of claims is right-skewed, with a large number of customers having no claims, while the number of customers decreases as claim amounts increase. Notably, there is a spike in vehicle claims between $4000\n",
        "\n",
        "and\n",
        "\n",
        "$5000, which also influences the overall claim distribution since vehicle claims make up the largest portion of total claims.\n",
        "\n",
        "**Correlation Heatmap**\n",
        "\"\"\"\n",
        "\n",
        "#select numerical columns\n",
        "numerical_columns = insurance_df.select_dtypes(include=['int64','float64']).columns\n",
        "#correlation matrix\n",
        "correlation_matrix = insurance_df[numerical_columns].corr()\n",
        "plt.figure(figsize=(25,25))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.show()\n",
        "\n",
        "\"\"\"The correlation matrix shows a high correlation between total_claim_amount and injury_claim, property_claim, and vehicle_claim, as total_claim_amount is simply their sum. Given this redundancy, the total_claim_amount column can be dropped to avoid multicollinearity.\n",
        "\n",
        "**Scatterplot**\n",
        "\"\"\"\n",
        "\n",
        "sns.pairplot(insurance_df, kind=\"scatter\", diag_kind=None)\n",
        "plt.figure(figsize=(40,40))\n",
        "plt.show()\n",
        "\n",
        "\"\"\"**Boxplot Analysis for Outlier Detection**\"\"\"\n",
        "\n",
        "# Outlier Detection using Boxplots\n",
        "# Calculate the number of rows needed for the subplots\n",
        "num_rows = (len(numerical_columns) + 2) // 3\n",
        "\n",
        "# Adjust figure size to accommodate all subplots and prevent overlap\n",
        "plt.figure(figsize=(15, 5 * num_rows))\n",
        "\n",
        "# Create boxplots for each column in subplots\n",
        "for i, col in enumerate(numerical_columns):\n",
        "    plt.subplot(num_rows, 3, i + 1)\n",
        "    # Using plt.boxplot to create the boxplot for the column\n",
        "    plt.boxplot(insurance_df[col])\n",
        "    plt.title(col)\n",
        "    plt.xticks(rotation=45)  # Rotate x-axis labels\n",
        "\n",
        "plt.suptitle(\"Outlier Detection using Boxplots\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "\"\"\"The majority of data points fall within the interquartile range (IQR), suggesting that outliers have minimal impact on the dataset.\n",
        "\n",
        "**Summary of Categorical Variables**\n",
        "\"\"\"\n",
        "\n",
        "#Extract categorical variable columns\n",
        "categorical_columns = insurance_df.select_dtypes(include=['object']).columns\n",
        "\n",
        "for col in categorical_columns:\n",
        "    print(f\"Feature: {col}\")\n",
        "    print(insurance_df[col].value_counts())\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "\"\"\"**Class Imbalance**\"\"\"\n",
        "\n",
        "#Calculate class imbalance\n",
        "insurance_df['fraud_reported'].value_counts()\n",
        "\n",
        "\"\"\"The count of records reported as fraud indicate there is class imbalance. This needs to be addressed during classification through class imbalance handling techniques.\n",
        "\n",
        "####**Model Building**\n",
        "\n",
        "**Feature Selection**\n",
        "\"\"\"\n",
        "\n",
        "#Remove unnecessary columns to eliminate irrelevant or redundant data, reducing overfitting, and speeding up the computation process.\n",
        "insurance_df.drop(['age', 'policy_bind_date', 'policy_number','policy_csl','policy_deductable','insured_relationship','incident_hour_of_the_day','umbrella_limit', 'insured_zip','insured_hobbies', 'capital-gains', 'capital-loss', 'incident_city', 'incident_date', 'incident_location', 'witnesses','number_of_vehicles_involved', 'auto_make', 'auto_year'], axis=1, inplace=True)\n",
        "insurance_df.head()\n",
        "\n",
        "\"\"\"**Encoding Categorical Variables**\"\"\"\n",
        "\n",
        "# Extract columns with categorical variables from updated data\n",
        "categorical_columns = insurance_df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Perform Label Encoding to covert text to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "for col in categorical_columns:\n",
        "    insurance_df[col] = label_encoder.fit_transform(insurance_df[col])\n",
        "\n",
        "insurance_df.head()\n",
        "\n",
        "\"\"\"**Split model into train and test set**\"\"\"\n",
        "\n",
        "#split model into train and test\n",
        "X = insurance_df.drop('fraud_reported', axis=1)\n",
        "y = insurance_df['fraud_reported']\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\"\"\"**Model 1: Logistic Regression**\"\"\"\n",
        "\n",
        "# Build a pipeline with SMOTE and Logistic Regression\n",
        "\n",
        "# Define scoring metrics\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score)\n",
        "}\n",
        "\n",
        "pipeline = ImbPipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('smote', SMOTE()),\n",
        "    ('classifier', LogisticRegression(class_weight='balanced'))\n",
        "])\n",
        "\n",
        "# Define cross-validation strategy\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_results = cross_validate(pipeline, x_train, y_train, cv=cv, scoring=scoring)\n",
        "\n",
        "# Display results\n",
        "print(\"Cross-Validation Results:\")\n",
        "for metric in scoring.keys():\n",
        "    print(f\"{metric}: {cv_results[f'test_{metric}'].mean():.4f}\")\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(x_test)\n",
        "\n",
        "#Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\"\"\"**Model 2: Random Forest Algorithm**\"\"\"\n",
        "\n",
        "#performing second model using RandomForestClassifier\n",
        "pipeline = ImbPipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('smote', SMOTE()),\n",
        "    ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42))\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(x_test)\n",
        "\n",
        "#Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\"\"\"**Model 3: XGBoost Classifier**\"\"\"\n",
        "\n",
        "# Count class distribution in training data\n",
        "class_0_count = sum(y_train == 0)\n",
        "class_1_count = sum(y_train == 1)\n",
        "\n",
        "# Calculate the weight\n",
        "scale_pos_weight = class_0_count / class_1_count\n",
        "model = XGBClassifier(scale_pos_weight=class_0_count / class_1_count)\n",
        "\n",
        "#initialize and train the model\n",
        "xgb_model = XGBClassifier(scale_pos_weight=scale_pos_weight, eval_metric='logloss')\n",
        "xgb_model.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "y_probs = xgb_model.predict_proba(x_test)[:, 1]\n",
        "threshold = 0.3\n",
        "y_pred = (y_probs >= threshold).astype(int)\n",
        "\n",
        "#Confustion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "#Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "#perform SHAP analysis to determine variable importance\n",
        "import shap\n",
        "\n",
        "# Initialize the SHAP explainer\n",
        "explainer = shap.TreeExplainer(xgb_model)\n",
        "\n",
        "# Calculate SHAP values for the test set\n",
        "shap_values = explainer.shap_values(x_test)\n",
        "\n",
        "#plot\n",
        "shap.summary_plot(shap_values, x_test)\n",
        "\n",
        "\"\"\"**Model 4: Ensemble Model**\"\"\"\n",
        "\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)),\n",
        "    ('xgb', XGBClassifier(eval_metric='logloss', random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42))\n",
        "], voting='soft')\n",
        "\n",
        "ensemble.fit(x_train, y_train)\n",
        "\n",
        "y_probs = ensemble.predict_proba(x_test)[:, 1]\n",
        "y_pred = (y_probs >= 0.4).astype(int)\n",
        "\n",
        "#Confustion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "#Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\"\"\"**Model 5: Feedforward Neural Network**\"\"\"\n",
        "\n",
        "# Preprocess: scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(x_train)\n",
        "X_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
        "\n",
        "# Split training into train and validation sets\n",
        "full_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=10)\n",
        "\n",
        "# Define the model\n",
        "class FNNClassifier(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(FNNClassifier, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "model = FNNClassifier(X_train_scaled.shape[1])\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Train the model with early stopping\n",
        "epochs = 100\n",
        "patience = 10\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_x).squeeze()\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for val_x, val_y in val_loader:\n",
        "            val_outputs = model(val_x).squeeze()\n",
        "            val_loss += criterion(val_outputs, val_y).item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping condition\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        best_model_state = model.state_dict()\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(best_model_state)\n",
        "\n",
        "# Evaluate on test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_prob = model(X_test_tensor).squeeze().numpy()\n",
        "    y_pred = (y_pred_prob > 0.35).astype(int)\n",
        "\n",
        "#Confustion Matrix\n",
        "cm = confusion_matrix(y_test.values, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "#Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test.values, y_pred))\n",
        "\n",
        "\"\"\"**Model Comparison**\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the model comparison metrics\n",
        "models = ['Logistic Regression', 'Random Forest', 'XGBoost', 'Ensemble Model', 'Neural Network']\n",
        "metrics = {\n",
        "    'Accuracy': [0.73, 0.77, 0.74, 0.77, 0.75],\n",
        "    'Precision': [0.69, 0.71, 0.68, 0.71, 0.69],\n",
        "    'Recall': [0.73, 0.69, 0.68, 0.72, 0.68],\n",
        "    'F1 Score': [0.70, 0.70, 0.68, 0.71, 0.68]\n",
        "}\n",
        "\n",
        "# Create a DataFrame for comparison\n",
        "model_comparison_df = pd.DataFrame(metrics, index=models)\n",
        "\n",
        "# Print the table\n",
        "model_comparison_df\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the model comparison metrics for radar chart\n",
        "labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "log_reg = [0.73, 0.69, 0.73, 0.70]  # Logistic Regression\n",
        "rf = [0.77, 0.71, 0.69, 0.70]      # Random Forest\n",
        "xgb = [0.74, 0.68, 0.68, 0.68]     # XGBoost\n",
        "ensemble = [0.77, 0.71, 0.72, 0.71]  # Ensemble Model\n",
        "nn = [0.75, 0.69, 0.68, 0.68]      # Neural Network\n",
        "\n",
        "# Combine all models\n",
        "data = np.array([log_reg, rf, xgb, ensemble, nn])\n",
        "\n",
        "# Number of variables\n",
        "num_vars = len(labels)\n",
        "\n",
        "# Compute angle for each axis\n",
        "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
        "\n",
        "# Make the plot circular\n",
        "data = np.concatenate((data, data[:,[0]]), axis=1)\n",
        "angles += angles[:1]\n",
        "\n",
        "# Create figure\n",
        "fig, ax = plt.subplots(figsize=(6, 6), dpi=80, subplot_kw=dict(polar=True))\n",
        "\n",
        "# Plot each model's data\n",
        "for i, model in enumerate(['Logistic Regression', 'Random Forest', 'XGBoost', 'Ensemble Model', 'Neural Network']):\n",
        "    ax.plot(angles, data[i], label=model, linewidth=2, linestyle='solid')\n",
        "\n",
        "# Fill area\n",
        "ax.fill(angles, data.T, alpha=0.1)\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_yticklabels([])\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_title('Model Comparison - Performance Metrics')\n",
        "\n",
        "# Display the legend\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.1))\n",
        "\n",
        "# Show the radar chart\n",
        "plt.show()\n",
        "\n",
        "\"\"\"**Modeling Summary:**\n",
        "\n",
        "Five different models were evaluated to identify the best classifier for the data. These models included Logistic Regression, Random Forest, XGBoost, an Ensemble model combining Logistic Regression, Random Forest, and XGBoost, and a Feedforward Neural Network.\n",
        "\n",
        "The accuracy of all models ranged from 60% to 70%. Logistic Regression showed decent performance, while Random Forest and XGBoost performed well individually.\n",
        "\n",
        "The ensemble model delivered the best performance with an accuracy of 77%, precision of 71%, recall of 72%, and an F1 score of 71%.\n",
        "\n",
        "The Feedforward Neural Network model also performed well, yielding results similar to the ensemble model, making it a strong contender for the final recommendation.\n",
        "\n",
        "Given its flexibility and the ability to tune parameters of the individual models, we recommend using the ensemble model for final deployment.\n",
        "\n",
        "**Final Recommendation:** Ensemble Model with Logistic Regression, Random Forest and XGBoost can be used for detecting insurance fraud.\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "\t1.\tThe overall accuracy, precision, recall, and F1 scores are in the 70% range, with even lower performance for the fraud class. This indicates that the model is not efficient in correctly identifying fraudulent claims, which is critical for fraud detection systems. The imbalance between fraudulent and non-fraudulent claims likely contributes to this limitation, as the models are more inclined to predict the majority class (non-fraud) due to the class distribution.\n",
        "\t2.\tFeature Selection: While some features were selected, the model may still benefit from additional feature engineering, as important patterns could be hidden within less obvious features.\n",
        "\n",
        "**Future Scope for Improvement:**\n",
        "\n",
        "\t1.\tAdvanced Feature Engineering: More advanced techniques such as feature scaling, polynomial feature generation, and interaction terms could be explored to capture complex relationships between the features.\n",
        "\t2.\tHyperparameter Optimization: More advanced hyperparameter tuning techniques like Randomized Search or Bayesian Optimization could be used to find the best settings for each model, particularly for more complex models like XGBoost and Neural Networks.\n",
        "\t3.\tEnsemble Methods: The current ensemble model combines basic models, but more sophisticated ensemble techniques (e.g., stacking) could be explored to improve the final prediction accuracy.\n",
        "\t4.\tReal-time Prediction: The project could evolve into a real-time fraud detection system that dynamically updates the models as new claims come in, making the system more responsive and effective at catching fraudulent claims in real-time.\n",
        "\"\"\"\n",
        "\n",
        "#import insurance_claims data as df\n",
        "df = pd.read_csv('insurance_claims.csv')\n",
        "\n",
        "# List of columns that were label-encoded\n",
        "label_encoded_cols = ['policy_state', 'incident_state', 'insured_sex', 'insured_education_level',\n",
        "                      'insured_occupation',\n",
        "                      'incident_type', 'collision_type', 'incident_severity',\n",
        "                      'authorities_contacted', 'property_damage',\n",
        "                      'police_report_available', 'auto_model']\n",
        "\n",
        "# Create a copy of x_test to decode and export\n",
        "decoded_x_test = x_test.copy()\n",
        "\n",
        "# For each categorical column, re-fit a LabelEncoder on insurance_df and decode x_test\n",
        "for col in label_encoded_cols:\n",
        "    le = LabelEncoder()\n",
        "    le.fit(df[col])  # Fit encoder on full dataset\n",
        "    decoded_x_test[col] = le.inverse_transform(x_test[col])\n",
        "\n",
        "tableau_df = decoded_x_test.copy()\n",
        "tableau_df['fraud_reported'] = y_test.values\n",
        "tableau_df['predicted_fraud'] = y_pred\n",
        "tableau_df['fraud_probability'] = y_probs\n",
        "\n",
        "\n",
        "tableau_df['actual_fraud'] = y_test.values\n",
        "tableau_df.head()\n",
        "\n",
        "tableau_df.to_csv('tableau_df.csv', index=False)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}